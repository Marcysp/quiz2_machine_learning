{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcysp/quiz2_machine_learning/blob/main/03_Alvina%20Marcy_kuis2_OCR_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "fLV1Z5_Za3Ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUhYpAeEarrV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #Mengimpor TensorFlow untuk machine learning.\n",
        "import numpy as np      #Mengimpor NumPy untuk operasi numerik.\n",
        "import pandas as pd     #Mengimpor Pandas untuk manipulasi data.\n",
        "import matplotlib.pyplot as plt     #Mengimpor Matplotlib untuk visualisasi data.\n",
        "import seaborn as sns               #Mengimpor Seaborn untuk visualisasi data statistik.\n",
        "from sklearn.preprocessing import LabelBinarizer    #Mengimpor LabelBinarizer dari scikit-learn untuk encoding variabel target.\n",
        "from sklearn.model_selection import train_test_split    #Mengimpor train_test_split dari scikit-learn untuk membagi data menjadi set pelatihan dan pengujian.\n",
        "from sklearn.metrics import classification_report       #Mengimpor classification_report dari scikit-learn untuk mengevaluasi hasil klasifikasi model.\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "N95oXMaTcnMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load MNIST\n",
        "\n",
        "Proses memuat dataset MNIST melibatkan penggunaan pustaka atau fungsi yang telah disediakan oleh framework seperti Keras atau TensorFlow. Fungsi tersebut digunakan untuk mengambil dataset MNIST dari sumbernya dan memasukkannya ke dalam lingkungan pengembangan, seperti Google Colab."
      ],
      "metadata": {
        "id": "dT8TY908cpCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengimpor dataset MNIST dari library TensorFlow Keras\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "OIZx2pkUcfDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mengambil data pelatihan dan pengujian dari kumpulan data MNIST dan menyimpannya dalam variabel\n",
        "# Variabel train_data dan train_labels berisi gambar dan labelnya untuk data pelatihan\n",
        "# Variabel test_data dan test_labels berisi gambar dan labelnya untuk data pengujian\n",
        " (train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "3GLkzBi6czj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape data\n",
        "(train_data.shape, test_data.shape)"
      ],
      "metadata": {
        "id": "oVQTyPgAc5Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape labels\n",
        "(train_labels.shape, test_labels.shape)"
      ],
      "metadata": {
        "id": "grUhLPwJc-un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check each data shape --> should be 28*28\n",
        "train_data[0].shape"
      ],
      "metadata": {
        "id": "AwsTEu6xdNKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the label\n",
        "train_labels.shape"
      ],
      "metadata": {
        "id": "KHZesdnBdXKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine Train and Test Data\n",
        "\n",
        "Dalam dataset MNIST, terdapat dua kumpulan data: data pelatihan (train) dan data pengujian (test). Biasanya, kedua kumpulan data ini digunakan secara terpisah untuk melatih dan menguji model.\n",
        "\n",
        "Namun, dalam beberapa situasi, ada kebutuhan untuk menggabungkan kedua set data menjadi satu dataset tunggal. Hal ini dapat dilakukan dengan maksud tertentu, misalnya, untuk menggabungkan data pelatihan dan pengujian ke dalam satu dataset yang lebih besar untuk meningkatkan pelatihan model."
      ],
      "metadata": {
        "id": "SYz8IjnOdfsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengkombinasikan data pelatihan dan data pengujian dari dataset MNIST ke dalam satu larik menggunakan np.vstack\n",
        "# Variabel digits_data akan memuat hasil gabungan dari gambar-gambar dari train_data dan test_data\n",
        "digits_data = np.vstack([train_data, test_data])\n",
        "\n",
        "# Menggabungkan label-label dari data pelatihan dan data pengujian dari dataset MNIST ke dalam satu larik menggunakan np.hstack\n",
        "# Variabel digits_labels akan berisi hasil gabungan dari label-label dari train_labels dan test_label\n",
        "digits_labels = np.hstack([train_labels, test_labels])"
      ],
      "metadata": {
        "id": "yacSZjdPdbhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data shape\n",
        "digits_data.shape"
      ],
      "metadata": {
        "id": "oRBvQNVUeHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check label shape\n",
        "digits_labels.shape"
      ],
      "metadata": {
        "id": "MZbUcu0MeLSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly checking the data\n",
        "#  Menghasilkan indeks acak antara 0 dan jumlah total gambar dalam dataset `digits_data`\n",
        "idx = np.random.randint(0, digits_data.shape[0])\n",
        "# Menampilkan gambar dengan indeks yang dihasilkan secara acak menggunakan plt.imshow().\n",
        "# Penggunaan cmap='gray' bertujuan untuk menampilkan gambar dalam skala warna abu-abu karena dataset MNIST berupa gambar grayscale.\n",
        "plt.imshow(digits_data[idx], cmap='gray')\n",
        "# Menampilkan judul plot yang berisi kelas atau label dari gambar yang dipilih secara acak\n",
        "# Menggunakan str() untuk mengonversi label ke dalam string sebelum menambahkannya ke judul plot\n",
        "plt.title('Class: ' + str(digits_labels[idx]))"
      ],
      "metadata": {
        "id": "uqaiC0pGePF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data distribution\n",
        "df_labels = pd.DataFrame(digits_labels, columns=['Labels'])\n",
        "# Menggunakan sns.countplot(df_labels, x='Labels') akan menghasilkan plot batang (countplot) yang menampilkan\n",
        "# distribusi frekuensi dari nilai-nilai pada kolom 'Labels' dalam DataFrame df_labels.\n",
        "# Setiap batang pada plot akan merepresentasikan jumlah kemunculan setiap nilai label pada sumbu x.\n",
        "# Dengan kata lain, ini memberikan gambaran visual tentang seberapa sering setiap nilai label muncul dalam dataset.\n",
        "sns.countplot(df_labels, x='Labels')"
      ],
      "metadata": {
        "id": "FrLWteorevqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Kaggle A-Z\n",
        "\n",
        "Dataset A-Z Handwritten Data adalah dataset yang berisi gambar tulisan tangan dari huruf A sampai Z. Setiap gambar menunjukkan satu huruf. Dataset ini dapat ditemukan di platform seperti Kaggle dan dapat digunakan untuk tugas-tugas pengenalan karakter atau OCR."
      ],
      "metadata": {
        "id": "uqeJT2ytogB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://iaexpert.academy/arquivos/alfabeto_A-Z.zip"
      ],
      "metadata": {
        "id": "tQ95CkdfjO7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract zip file\n",
        "# Membuka file zip dengan mode 'read' (mode='r')\n",
        "zip_object = zipfile.ZipFile(file = 'alfabeto_A-Z.zip', mode = 'r')\n",
        "# Mengekstrak semua file dalam objek zip ke dalam direktori yang ditentukan.\n",
        "# Dalam hal ini, ./ menunjukkan bahwa file akan diekstrak ke dalam direktori saat ini.\n",
        "zip_object.extractall('./')\n",
        "# Menutup objek zip setelah proses ekstraksi selesai.\n",
        "# Ini adalah langkah yang baik untuk memastikan bahwa semua sumber daya terkait dengan objek zip dibersihkan dan membebaskan memori setelah selesai digunakan.\n",
        "zip_object.close()"
      ],
      "metadata": {
        "id": "TobpFNSsojcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghubungkan Google Drive (pada lingkungan Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TqtFgQt0u7s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan Pandas untuk membaca dataset dari file CSV yang bernama 'A_Z Handwritten Data.csv'.\n",
        "# kemudian Mengonversi tipe data semua kolom dalam DataFrame menjadi float32. Hal ini dapat bermanfaat untuk efisiensi memori\n",
        "# jika dataset berisi nilai numerik yang dapat direpresentasikan sebagai bilangan pecahan 32-bit.\n",
        "dataset_az = pd.read_csv('A_Z Handwritten Data.csv').astype('float32')\n",
        "# Menampilkan DataFrame dataset_az yang telah dibaca dan dikonversi tipenya\n",
        "dataset_az"
      ],
      "metadata": {
        "id": "ScBURU_4o5bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get pixel data only\n",
        "alphabet_data = dataset_az.drop('0', axis=1)\n",
        "# Get labels only\n",
        "alphabet_labels = dataset_az['0']"
      ],
      "metadata": {
        "id": "dK6pP0b-o81T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape data\n",
        "alphabet_data.shape, alphabet_labels.shape"
      ],
      "metadata": {
        "id": "NllEoaiLpZhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape labels\n",
        "alphabet_labels.shape"
      ],
      "metadata": {
        "id": "UvE88zSkpe5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape pixel data to 28*28\n",
        "alphabet_data = np.reshape(alphabet_data.values, (alphabet_data.shape[0], 28, 28))\n",
        "# Check the result by its shape\n",
        "alphabet_data.shape"
      ],
      "metadata": {
        "id": "9xps-5kjpio_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly checking A-Z dataset\n",
        "index = np.random.randint(0, alphabet_data.shape[0])\n",
        "plt.imshow(alphabet_data[index], cmap = 'gray')\n",
        "plt.title('Class: ' + str(alphabet_labels[index]));"
      ],
      "metadata": {
        "id": "tDTBbWSoqloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data distribution\n",
        "# Membuat DataFrame dari label-label (alphabet_labels) dengan satu kolom bernama 'Labels'\n",
        "df_az_labels = pd.DataFrame({\n",
        "    'Labels': alphabet_labels.values\n",
        "})\n",
        "# Menggunakan Seaborn untuk membuat plot distribusi label menggunakan countplot\n",
        "# x='Labels' menunjukkan bahwa sumbu x akan berisi data dari kolom 'Labels' pada DataFrame\n",
        "sns.countplot(df_az_labels, x='Labels')"
      ],
      "metadata": {
        "id": "72frSp_uqrjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine Dataset (MNIST + Kaggel A-Z)\n",
        "\n",
        "Langkah ini melibatkan menggabungkan dataset MNIST yang berisi gambar digit tulisan tangan dengan dataset Kaggle A-Z yang berisi gambar huruf tulisan tangan. Hasilnya adalah pembentukan satu dataset yang lebih besar dan lebih beragam. Tujuan dari langkah ini adalah untuk melatih model yang memiliki kekuatan dan keberagaman yang lebih baik dengan menggabungkan informasi dari dua dataset yang memiliki karakteristik berbeda."
      ],
      "metadata": {
        "id": "YZf14K79sdEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique value from digits_labels\n",
        "np.unique(digits_labels)"
      ],
      "metadata": {
        "id": "9voZzZ8Krqk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique value from alphabet_labels\n",
        "np.unique(alphabet_labels)"
      ],
      "metadata": {
        "id": "9ZzBJFKxs1AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We already know that digits labels containt labels from 0-9 (10 labels)\n",
        "# We also know that alphabet labels start from 0-25 which represent A-Z\n",
        "# If we want to combine them, the A-Z labels should continuing the digits label\n",
        "\n",
        "alphabet_labels += 10\n",
        "# Menambahkan 10 ke setiap label pada alphabet_labels untuk melanjutkan dari 10 ke atas (untuk A-Z)"
      ],
      "metadata": {
        "id": "dM0GV8lIsnzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cek kembali nilai alphabet_labels\n",
        "np.unique(alphabet_labels)"
      ],
      "metadata": {
        "id": "42YKtJB-tQC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both of them\n",
        "#  Menggabungkan kedua dataset (alphabet_data dan digits_data)\n",
        "data = np.vstack([alphabet_data, digits_data])\n",
        "labels = np.hstack([alphabet_labels, digits_labels])"
      ],
      "metadata": {
        "id": "1ydXPXiItVVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape\n",
        "# cek ukuran dari bentuk data\n",
        "data.shape, labels.shape"
      ],
      "metadata": {
        "id": "-vYLoXFjuLz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check labels\n",
        "# Cek nilai-nilai yang unik dari labels yang baru digabungkan\n",
        "np.unique(labels)"
      ],
      "metadata": {
        "id": "biyM6WueuRG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to float32\n",
        "data = np.array(data, dtype = 'float32')"
      ],
      "metadata": {
        "id": "vuEt24lhuVfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since Convolutional need 3d data (including depth)\n",
        "# and our images only in 2d data (because in grayscale format)\n",
        "# we need to add \"the depth\" to the data\n",
        "data = np.expand_dims(data, axis=-1)\n",
        "\n",
        "# check shape  dari data setelah ditambah dimensi kedalaman\n",
        "data.shape"
      ],
      "metadata": {
        "id": "UC6Vhz59ucdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Langkah ini mencakup berbagai teknik untuk menyiapkan data sebelum digunakan dalam model."
      ],
      "metadata": {
        "id": "xF71lXtSu3YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "# Membagi setiap nilai dalam variabel data dengan 255.0.\n",
        "data /= 255.0"
      ],
      "metadata": {
        "id": "kFjjP_JLuzbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check range value of data\n",
        "# mengecek rentang nilai dari elemen-elemen pada indeks pertama dari variabel 'data'.\n",
        "data[0].min(), data[0].max()"
      ],
      "metadata": {
        "id": "zztStKZOu-Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enconde the labels\n",
        "# LabelBinarizer similar with OneHotEncoder\n",
        "# Menggunakan LabelBinarizer untuk mengonversi label-label dalam variabel 'labels' menjadi bentuk biner.\n",
        "# LabelBinarizer digunakan untuk mengubah kategori label menjadi representasi biner.\n",
        "le = LabelBinarizer()\n",
        "labels = le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "s2C5yKEGu7vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check labels shape\n",
        "# Menggunakan LabelBinarizer untuk mengonversi label-label dalam variabel 'labels' menjadi bentuk biner.\n",
        "# LabelBinarizer digunakan untuk mengubah kategori label menjadi representasi biner.\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "Bo1M5YEmvg5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data with label binarizer's label\n",
        "plt.imshow(data[30000].reshape(28,28), cmap='gray')\n",
        "plt.title(str(labels[0]))\n",
        "# menampilkan gambar dari data pada indeks ke-30000 (asumsi data berbentuk\n",
        "# gambar dengan dimensi 28x28) beserta label biner yang sesuai."
      ],
      "metadata": {
        "id": "jCL5ipbsvjpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since our data is not balance, we will handle it by giving weight for 'small' data\n",
        "\n",
        "# Check number of data for each labels first\n",
        "classes_total = labels.sum(axis = 0)\n",
        "classes_total\n",
        "# menghitung jumlah data untuk setiap label pertama dengan menjumlahkan\n",
        "# nilai-nilai pada setiap kolom dari variabel 'labels'."
      ],
      "metadata": {
        "id": "njDbZXKzvsx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the biggest value of data\n",
        "classes_total.max()\n",
        "# menampilkan nilai maksimum dari jumlah data pada satu label tertentu."
      ],
      "metadata": {
        "id": "R1hHYtGLv_ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a weight for each data\n",
        "classes_weights = {}\n",
        "for i in range(0, len(classes_total)):\n",
        "  #print(i)\n",
        "  classes_weights[i] = classes_total.max() / classes_total[i]\n",
        "# Membuat bobot untuk setiap data dengan melakukan iterasi pada setiap label dan menghitung bobotnya\n",
        "# berdasarkan perbandingan jumlah data terbanyak dengan jumlah data pada setiap label.\n",
        "\n",
        "# Check the weight for each data\n",
        "classes_weights\n",
        "# menampilkan bobot yang telah dihitung untuk setiap data."
      ],
      "metadata": {
        "id": "7TJFrxS2wFTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### penjelasan\n",
        "\n",
        "Proses normalisasi data dilakukan untuk mengubah rentang nilai setiap fitur data menjadi kisaran antara 0 hingga 1. Hal ini bertujuan untuk menyamakan skala nilai antar fitur, memudahkan model dalam memahami pola-pola dalam dataset. Sebagai contoh, normalisasi dapat mengubah intensitas piksel dalam gambar dari rentang 0 hingga 255 menjadi rentang 0 hingga 1, sesuai dengan kebutuhan pelatihan model.\n",
        "\n",
        "Pada tahap encoding label, menggunakan Label Binarizer digunakan untuk mengonversi label kategori menjadi representasi biner. Hasilnya adalah matriks biner yang merepresentasikan setiap label dalam dataset. Matriks ini memiliki jumlah kolom sesuai dengan jumlah kelas atau label dalam dataset, dan setiap baris mewakili satu sampel data.\n",
        "\n",
        "Tampilan gambar yang ditampilkan merupakan representasi visual dari dataset. Gambar tersebut berasosiasi dengan label biner tertentu, seperti [0, 1, 0]. Menampilkan gambar dengan label biner ini memungkinkan visualisasi data berdasarkan labelnya.\n",
        "\n",
        "Pemberian bobot untuk setiap label bertujuan untuk menangani ketidakseimbangan data. Dengan memberikan bobot, model lebih fokus pada data yang jumlahnya lebih sedikit, memungkinkan pembelajaran yang lebih baik dari data yang kurang representatif.\n",
        "\n",
        "Dengan demikian, hasil dari proses preprocessing mencakup normalisasi data, encoding label menggunakan Label Binarizer, tampilan visual dari dataset berdasarkan label biner, dan pemberian bobot untuk menangani ketidakseimbangan data. Ini bertujuan untuk mempersiapkan data dengan cara yang memungkinkan model untuk belajar dengan efektif."
      ],
      "metadata": {
        "id": "WMNdJgqnwm-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data to Train and Test\n",
        "\n",
        "membagi dataset menjadi data yang digunakan sebagai latihan oleh mesin dan data uji. Data latih digunakan untuk melatih model, sedangkan data uji digunakan untuk menguji kinerja model yang telah dilatih."
      ],
      "metadata": {
        "id": "VtDS1O6JwSQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 1, stratify = labels)"
      ],
      "metadata": {
        "id": "rV62GJySwM2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Augmentation"
      ],
      "metadata": {
        "id": "q7Hs5GwGxw4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang dibutuhkan untuk augmentasi data menggunakan ImageDataGenerator dari TensorFlow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "Xn1jiVn9x2Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageDataGenerator menghasilkan data variasi baru dari data yang sudah ada dengan menerapkan transformasi khusus\n",
        "# Transformasi tersebut dapat mencakup rotasi, zoom, pergeseran, dan flip horizontal, yang dapat diterapkan pada gambar.\n",
        "augmentation = ImageDataGenerator(rotation_range = 10, zoom_range=0.05, width_shift_range=0.1,\n",
        "                                  height_shift_range=0.1, horizontal_flip = False)"
      ],
      "metadata": {
        "id": "su41P3box6Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build CNN Model"
      ],
      "metadata": {
        "id": "GYwIT1TZw8SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "\n",
        "# import library yang dibutuhkan untuk membangun model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "nvQ2Ly4zwX8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the network\n",
        "network = Sequential()\n",
        "\n",
        "# Menambahkan layer konvolusi dengan 32 filter, kernel size 3x3, fungsi aktivasi ReLU, dan input shape 28x28x1\n",
        "network.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
        "# Menambahkan layer max pooling dengan pool size 2x2\n",
        "network.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "# Menambahkan layer konvolusi dengan 64 filter, kernel size 3x3, fungsi aktivasi ReLU, dan padding 'same'\n",
        "network.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# Menambahkan layer max pooling dengan pool size 2x2\n",
        "network.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "# Menambahkan layer konvolusi dengan 128 filter, kernel size 3x3, fungsi aktivasi ReLU, dan padding 'valid'\n",
        "network.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
        "# Menambahkan layer max pooling dengan pool size 2x2\n",
        "network.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "# Meratakan output menjadi satu dimensi\n",
        "network.add(Flatten())\n",
        "\n",
        "# Menambahkan layer dense (fully connected) dengan 64 neuron dan fungsi aktivasi ReLU\n",
        "network.add(Dense(64, activation='relu'))\n",
        "# Menambahkan layer dense dengan 128 neuron dan fungsi aktivasi ReLU\n",
        "network.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Menambahkan layer output dengan 36 neuron (sesuai jumlah kelas), menggunakan fungsi aktivasi softmax\n",
        "network.add(Dense(36, activation='softmax'))\n",
        "\n",
        "# Mengkompilasi model dengan categorical crossentropy loss, optimizer Adam, dan metrik akurasi\n",
        "network.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "icRreUChxAwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check network summary\n",
        "# melihat ringkasan atau struktur dari model jaringan\n",
        "network.summary()"
      ],
      "metadata": {
        "id": "nAuCYbRWxIKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat label untuk kelas-kelas yang akan diprediksi\n",
        "name_labels = '0123456789'\n",
        "# Menambahkan label huruf besar A-Z ke dalam string name_labels\n",
        "name_labels += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "# Membuat list name_labels yang berisi karakter-karakter dari string name_labels\n",
        "name_labels = [l for l in name_labels]\n",
        "\n",
        "# Menampilkan label yang sebenarnya\n",
        "print(name_labels)\n"
      ],
      "metadata": {
        "id": "o7Vc4ly2xMhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "DH7hSW38xX6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model name, epoch, and batch size\n",
        "file_model = 'custom_ocr.model'\n",
        "epochs = 20\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "rNQ_-PY7xVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup checkpoint\n",
        "# Mengatur callback ModelCheckpoint untuk menyimpan model dengan performa terbaik selama pelatihan\n",
        "checkpointer = ModelCheckpoint(file_model, monitor = 'val_loss', verbose = 1, save_best_only=True)"
      ],
      "metadata": {
        "id": "F8Orj5UQxdtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = network.fit(\n",
        "    # Menggunakan augmented data dari generator dengan ukuran batch yang ditentukan\n",
        "    augmentation.flow(X_train, y_train, batch_size=batch_size),\n",
        "    # Data validasi yang tidak di-augmentasi\n",
        "    validation_data=(X_test, y_test),\n",
        "    # Menentukan jumlah langkah per epoch\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    # Jumlah epoch yang diinginkan\n",
        "    epochs=epochs,\n",
        "    # Menentukan bobot kelas untuk penanganan ketidakseimbangan\n",
        "    class_weight=classes_weights,\n",
        "    # Tampilkan informasi pelatihan secara detail\n",
        "    verbose=1,\n",
        "    # Menggunakan callback untuk menyimpan model terbaik\n",
        "    callbacks=[checkpointer]\n",
        ")\n"
      ],
      "metadata": {
        "id": "enFAwiaxxkgx",
        "outputId": "6adf66f6-935a-43b7-e9f3-9519da69d8b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 2.2416 - accuracy: 0.8201\n",
            "Epoch 1: val_loss improved from inf to 0.22301, saving model to custom_ocr.model\n",
            "2765/2765 [==============================] - 168s 55ms/step - loss: 2.2416 - accuracy: 0.8201 - val_loss: 0.2230 - val_accuracy: 0.9176\n",
            "Epoch 2/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 1.0016 - accuracy: 0.9000\n",
            "Epoch 2: val_loss did not improve from 0.22301\n",
            "2765/2765 [==============================] - 196s 71ms/step - loss: 1.0016 - accuracy: 0.9000 - val_loss: 0.2377 - val_accuracy: 0.9116\n",
            "Epoch 3/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.8365 - accuracy: 0.9112\n",
            "Epoch 3: val_loss did not improve from 0.22301\n",
            "2765/2765 [==============================] - 149s 54ms/step - loss: 0.8365 - accuracy: 0.9112 - val_loss: 0.2662 - val_accuracy: 0.8833\n",
            "Epoch 4/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.7403 - accuracy: 0.9194\n",
            "Epoch 4: val_loss did not improve from 0.22301\n",
            "2765/2765 [==============================] - 133s 48ms/step - loss: 0.7403 - accuracy: 0.9194 - val_loss: 0.2853 - val_accuracy: 0.8827\n",
            "Epoch 5/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.9244\n",
            "Epoch 5: val_loss improved from 0.22301 to 0.22184, saving model to custom_ocr.model\n",
            "2765/2765 [==============================] - 145s 52ms/step - loss: 0.6794 - accuracy: 0.9244 - val_loss: 0.2218 - val_accuracy: 0.9073\n",
            "Epoch 6/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.9295\n",
            "Epoch 6: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 135s 49ms/step - loss: 0.6315 - accuracy: 0.9295 - val_loss: 0.2496 - val_accuracy: 0.8999\n",
            "Epoch 7/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.9331\n",
            "Epoch 7: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 146s 53ms/step - loss: 0.5856 - accuracy: 0.9331 - val_loss: 0.2686 - val_accuracy: 0.8910\n",
            "Epoch 8/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.9351\n",
            "Epoch 8: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 140s 51ms/step - loss: 0.5645 - accuracy: 0.9351 - val_loss: 0.2572 - val_accuracy: 0.8920\n",
            "Epoch 9/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.5445 - accuracy: 0.9370\n",
            "Epoch 9: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 139s 50ms/step - loss: 0.5447 - accuracy: 0.9370 - val_loss: 0.3143 - val_accuracy: 0.8688\n",
            "Epoch 10/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.9383\n",
            "Epoch 10: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 138s 50ms/step - loss: 0.5140 - accuracy: 0.9383 - val_loss: 0.3032 - val_accuracy: 0.8621\n",
            "Epoch 11/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.4957 - accuracy: 0.9399\n",
            "Epoch 11: val_loss did not improve from 0.22184\n",
            "2765/2765 [==============================] - 131s 47ms/step - loss: 0.4956 - accuracy: 0.9399 - val_loss: 0.2688 - val_accuracy: 0.8807\n",
            "Epoch 12/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.9413\n",
            "Epoch 12: val_loss improved from 0.22184 to 0.21913, saving model to custom_ocr.model\n",
            "2765/2765 [==============================] - 141s 51ms/step - loss: 0.4847 - accuracy: 0.9413 - val_loss: 0.2191 - val_accuracy: 0.9111\n",
            "Epoch 13/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.4785 - accuracy: 0.9419\n",
            "Epoch 13: val_loss did not improve from 0.21913\n",
            "2765/2765 [==============================] - 129s 47ms/step - loss: 0.4785 - accuracy: 0.9419 - val_loss: 0.2221 - val_accuracy: 0.9020\n",
            "Epoch 14/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.9433\n",
            "Epoch 14: val_loss improved from 0.21913 to 0.15022, saving model to custom_ocr.model\n",
            "2765/2765 [==============================] - 140s 51ms/step - loss: 0.4543 - accuracy: 0.9433 - val_loss: 0.1502 - val_accuracy: 0.9461\n",
            "Epoch 15/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9443\n",
            "Epoch 15: val_loss did not improve from 0.15022\n",
            "2765/2765 [==============================] - 130s 47ms/step - loss: 0.4603 - accuracy: 0.9443 - val_loss: 0.1947 - val_accuracy: 0.9233\n",
            "Epoch 16/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.9451\n",
            "Epoch 16: val_loss did not improve from 0.15022\n",
            "2765/2765 [==============================] - 139s 50ms/step - loss: 0.4474 - accuracy: 0.9451 - val_loss: 0.1706 - val_accuracy: 0.9398\n",
            "Epoch 17/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.9463\n",
            "Epoch 17: val_loss did not improve from 0.15022\n",
            "2765/2765 [==============================] - 130s 47ms/step - loss: 0.4363 - accuracy: 0.9463 - val_loss: 0.1518 - val_accuracy: 0.9459\n",
            "Epoch 18/20\n",
            "2764/2765 [============================>.] - ETA: 0s - loss: 0.4224 - accuracy: 0.9478\n",
            "Epoch 18: val_loss did not improve from 0.15022\n",
            "2765/2765 [==============================] - 136s 49ms/step - loss: 0.4224 - accuracy: 0.9478 - val_loss: 0.1880 - val_accuracy: 0.9196\n",
            "Epoch 19/20\n",
            "2765/2765 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.9487\n",
            "Epoch 19: val_loss improved from 0.15022 to 0.12293, saving model to custom_ocr.model\n",
            "2765/2765 [==============================] - 133s 48ms/step - loss: 0.4134 - accuracy: 0.9487 - val_loss: 0.1229 - val_accuracy: 0.9635\n",
            "Epoch 20/20\n",
            " 515/2765 [====>.........................] - ETA: 2:09 - loss: 0.4158 - accuracy: 0.9487"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### penjelasan\n",
        "\n",
        "Hasil dari langkah ini adalah melatih model jaringan saraf dengan menerapkan augmentasi data pada dataset pelatihan. Teknik augmentasi data memungkinkan dataset asli diperkaya dengan variasi tambahan, seperti rotasi, pergeseran, dan perbesaran gambar, yang diterapkan secara dinamis selama proses pelatihan. Data latih mengalir melalui langkah-langkah (batches) yang dibentuk oleh aliran data augmentasi.\n",
        "\n",
        "Selama proses pelatihan, model diperbarui berulang kali melalui jumlah epoch yang telah ditentukan. Setiap epoch melibatkan iterasi melalui seluruh dataset latih. Penggunaan bobot kelas memberikan kesadaran tambahan kepada model terhadap kelas-kelas yang jumlahnya lebih sedikit dalam dataset, membantu model fokus pada kelas-kelas yang kurang representatif.\n",
        "\n",
        "Hasil akhir pelatihan dapat dilihat melalui objek history yang terbentuk. Objek ini berisi metrik seperti akurasi atau kehilangan (loss) yang terakumulasi selama proses pelatihan model. Analisis objek history memungkinkan evaluasi performa dan pembelajaran model pada setiap iterasi pelatihan.\n",
        "\n",
        "Secara keseluruhan, proses ini melibatkan pelatihan model jaringan saraf dengan memanfaatkan augmentasi data untuk memperluas variasi dataset, penggunaan bobot kelas untuk penekanan pada kelas-kelas minor, dan pemantauan serta evaluasi pelatihan model menggunakan callback yang mengamati performa selama proses pelatihan."
      ],
      "metadata": {
        "id": "fA30awaSzIqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "bM4mGoWL6jxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a Single Prediction"
      ],
      "metadata": {
        "id": "ig0TiV6N7V29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5pbMkmvg7V0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "# Melakukan prediksi menggunakan model neural network terhadap data uji (X_test)\n",
        "# network.predict() digunakan untuk membuat prediksi\n",
        "# batch_size=batch_size menunjukkan ukuran batch yang digunakan saat melakukan prediksi\n",
        "predictions = network.predict(X_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "WDJUSW-l6lEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan prediksi dari model untuk data dengan indeks ke-1 dari data uji (X_test)\n",
        "# predictions[1] digunakan untuk mengakses prediksi untuk data dengan indeks ke-1\n",
        "# Ini akan menampilkan probabilitas untuk setiap label\n",
        "predictions[1]"
      ],
      "metadata": {
        "id": "tQ5Ta4b96pEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the actual prediction -> highest probability\n",
        "np.argmax(predictions[1])"
      ],
      "metadata": {
        "id": "9THTrYAH64g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check label for 24\n",
        "name_labels[18]"
      ],
      "metadata": {
        "id": "pG6n2eTI6-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check y_test label for 0\n",
        "y_test[1]"
      ],
      "metadata": {
        "id": "chDSvyYq7Bxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the highest value\n",
        "np.argmax(y_test[1])"
      ],
      "metadata": {
        "id": "BpoE99Us7KpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the label of y_test 0\n",
        "name_labels[np.argmax(y_test[18])]"
      ],
      "metadata": {
        "id": "KjJRX1lC7Ppw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make an Evaluation on Test Data"
      ],
      "metadata": {
        "id": "_xFS3-cs7a0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "network.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "t-rko8WP7TYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Classification Report\n",
        "print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names = name_labels))"
      ],
      "metadata": {
        "id": "VkQtgoCq7gx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize loss value for each epoch\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "metadata": {
        "id": "mIETfqMu7nv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also check the another metrics\n",
        "history.history.keys()"
      ],
      "metadata": {
        "id": "mNUP1QP17vas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model performance by validation accuracy\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "VwEZzjEa716s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save The Model"
      ],
      "metadata": {
        "id": "D0rviDyl79Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The result will show in colab directory\n",
        "network.save('network', save_format= 'h5')"
      ],
      "metadata": {
        "id": "QZ4ftlmq76nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Real Image"
      ],
      "metadata": {
        "id": "A3EWZVrz0YDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "1wUedJB-0Z_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved network\n",
        "load_network = load_model('network')"
      ],
      "metadata": {
        "id": "D6E24QOm0hiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check summary\n",
        "load_network.summary()"
      ],
      "metadata": {
        "id": "0cB7lmOc0nlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = cv2.imread('b_small.png')\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "hbnaTxZK07JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape\n",
        "img.shape"
      ],
      "metadata": {
        "id": "EfDOYN6D1OZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to gray\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# check shape\n",
        "gray_img.shape"
      ],
      "metadata": {
        "id": "7e6w1zaL1RW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process\n",
        "# Binary Threshold and Otsu\n",
        "value, thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "cv2_imshow(thresh)\n",
        "\n",
        "# print threshold value\n",
        "print(value)"
      ],
      "metadata": {
        "id": "9_al0IF01c3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize image in order to match network input shape -> 28*28\n",
        "img_resize = cv2.resize(gray_img, (28,28))\n",
        "cv2_imshow(img_resize)"
      ],
      "metadata": {
        "id": "dmjHswqd1wB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to float 32\n",
        "# and extend the dimension since network input shape is 28*28*1\n",
        "img_input = img_resize.astype('float32') / 255 # also perform normalization\n",
        "img_input = np.expand_dims(img_input, axis=-1) # insert depth\n",
        "\n",
        "# check shape\n",
        "img_input.shape"
      ],
      "metadata": {
        "id": "bZStKgUl2Kk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add \"amount of data\" as dimension\n",
        "img_input = np.reshape(img_input, (1,28,28,1))\n",
        "img_input.shape"
      ],
      "metadata": {
        "id": "7Xs7JwKj2lxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a predition\n",
        "prediction = load_network.predict(img_input)\n",
        "pred_label = np.argmax(prediction) # predict actual label\n",
        "pred_label"
      ],
      "metadata": {
        "id": "fZLWIWLs2yuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check label for 6\n",
        "name_labels[6]"
      ],
      "metadata": {
        "id": "72Pl04hq3WNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2EmYPAN3hrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}